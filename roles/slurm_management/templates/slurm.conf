ClusterName={{ slurm_cluster_name }}
ControlMachine={{ hostvars[groups['sys_admin_interface'][0]]['ansible_hostname'] }}
ControlAddr={{ hostvars[groups['sys_admin_interface'][0]]['ansible_hostname'] }}
#BackupController=
#BackupAddr=
#
SlurmUser=slurm
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/var/spool/slurm
SlurmdSpoolDir=/var/spool/slurmd
SwitchType=switch/none
MpiDefault=none
MpiParams=ports=12000-12999
SlurmctldPidFile=/var/run/slurm/slurmctld.pid
SlurmdPidFile=/var/run/slurm/slurmd.pid
ProctrackType=proctrack/cgroup
#PluginDir=
CacheGroups=0
#FirstJobId=
ReturnToService=1
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
Prolog=/etc/slurm/slurm.prolog
PrologFlags=Alloc
Epilog=/etc/slurm/slurm.epilog*
#SrunProlog=
#SrunEpilog=
TaskProlog=/etc/slurm/slurm.taskprolog
#TaskEpilog=/etc/slurm/slurm.taskepilog
TaskPlugin=task/affinity,task/cgroup
JobSubmitPlugins=lua
#TrackWCKey=no
#TreeWidth=50
TmpFS=/local
UnkillableStepTimeout=180
#UsePAM=
#CheckpointType=checkpoint/blcr
#JobCheckpointDir=/var/slurm/checkpoint
# Terminate job immediately when one of the processes is crashed or aborted.
KillOnBadExit=1
# Automatically requeue jobs after a node failure or preemption by a higher prio job.
JobRequeue=1
# Cgroups already enforce resource limits, SLURM should not do this
MemLimitEnforce=no
#
# TIMERS
#
SlurmctldTimeout=300
SlurmdTimeout=300
MessageTimeout=60
GetEnvTimeout=20
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=15
#
# SCHEDULING
#
SchedulerType=sched/backfill
SchedulerPort=7321
SchedulerParameters=kill_invalid_depend,bf_continue,bf_max_job_test=10000,bf_max_job_user=5000,default_queue_depth=1000,bf_window=10080,bf_resolution=300,bf_busy_nodes,preempt_reorder_count=100,preempt_youngest_first
SelectType=select/cons_res
SelectTypeParameters=CR_Core_Memory
#SchedulerAuth=
#SchedulerRootFilter=
FastSchedule=1
PriorityType=priority/multifactor
PriorityDecayHalfLife=3-0
PriorityFavorSmall=NO
# Not necessary if there is a decay
#PriorityUsageResetPeriod=14-0
PriorityWeightAge=1000
PriorityWeightFairshare=100000
PriorityWeightJobSize=0
PriorityWeightPartition=0
PriorityWeightQOS=1000000
PriorityMaxAge=14-0
PriorityFlags=ACCRUE_ALWAYS,FAIR_TREE,MAX_TRES
PreemptType=preempt/qos
PreemptMode=REQUEUE
#
# Reservations
#
#ResvOverRun=UNLIMITED
#
# LOGGING
#
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurm/slurmd.log
JobCompType=jobcomp/filetxt
JobCompLoc=/var/log/slurm/slurm.jobcomp
#
# EMAIL NOTIFICATIONS
#
# Disable sending of email notifications, because it may cause a spam flood,
# when thousands of scripts with the same bug crash shortly after another.
# We also don't want jobs to fail when email notifications were requested,
# so therefore we alias the mail program to /bin/true
#
MailProg=/bin/true
#
# ACCOUNTING
#
#AcctGatherEnergyType=acct_gather_energy/rapl
#JobAcctGatherFrequency=energy=30
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherParams=UsePss,NoOverMemoryKill
# Users have to be in the accounting database
# (otherwise we don't have accounting records and fairshare)
AccountingStorageEnforce=limits,qos # will also enable: associations
#AcctGatherProfileType=acct_gather_profile/hdf5
#JobAcctGatherFrequency=30
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost={{ hostvars[groups['sys_admin_interface'][0]]['ansible_hostname'] }}
#AccountingStorageLoc=/var/log/slurm/slurm.accounting
#AccountingStoragePass=
#AccountingStorageUser=
MaxJobCount=100000
#
# Node Health Check (NHC)
#
HealthCheckProgram=/usr/sbin/nhc
HealthCheckInterval=300
#
# Partitions
#
EnforcePartLimits=YES
PartitionName=DEFAULT State=UP DefMemPerCPU=1024 MaxNodes={% if slurm_allow_jobs_to_span_nodes is defined and true %}{{ groups['compute_vm']|list|length }}{% else %}1{% endif %} MaxTime=7-00:00:01
PartitionName=regular Default=YES MaxNodes={% if slurm_allow_jobs_to_span_nodes is defined and true %}{{ groups['compute_vm']|list|length }}{% else %}1{% endif %} Nodes={{ vcompute_hostnames }} MaxCPUsPerNode={{ vcompute_max_cpus_per_node }} MaxMemPerNode={{ vcompute_max_mem_per_node }} TRESBillingWeights="CPU=1.0,Mem=0.125G" DenyQos=ds-short,ds-medium,ds-long
PartitionName=ds      Default=No  MaxNodes=1 Nodes={{ ui_hostnames }} MaxCPUsPerNode=1 MaxMemPerNode=1024 TRESBillingWeights="CPU=1.0,Mem=1.0G" AllowQos=ds-short,ds-medium,ds-long
#
# COMPUTE NODES
#
NodeName={{ vcompute_hostnames }} Sockets={{ vcompute_sockets }} CoresPerSocket={{ vcompute_cores_per_socket }} ThreadsPerCore=1 State=UNKNOWN RealMemory={{ vcompute_real_memory }} TmpDisk={{ vcompute_local_disk | default(0, true) }} Feature={{ vcompute_features }}
NodeName={{ ui_hostnames }}       Sockets={{ ui_sockets }}       CoresPerSocket={{ ui_cores_per_socket }}       ThreadsPerCore=1 State=UNKNOWN RealMemory={{ ui_real_memory }}       TmpDisk={{ ui_local_disk | default(0, true) }}       Feature={{ ui_features }}
