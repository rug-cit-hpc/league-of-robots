---
all:
  children:
    openstack_api:
      hosts:
        localhost:
          ansible_python_interpreter: /usr/bin/env python
    jumphost:
      hosts:
        tunnel:
          cloud_flavor: umcg.v1.vm.1-2
          host_networks:
            - name: "{{ stack_prefix }}_internal_management"
              security_group: "{{ stack_prefix }}_jumphosts"
              assign_floating_ip: true
          basic_security_ssh_challenge_response_auth: 'yes' # Required for MFS in sshd_config.
          local_yum_repository: true
    repo:
      hosts:
        nb-repo:
          cloud_flavor: umcg.v1.vm.1-2
          host_networks:
            - name: "{{ stack_prefix }}_internal_management"
              security_group: "{{ stack_prefix }}_repo"
          swap_file_size: 2
    data_transfer:
      hosts:
        nb-transfer:
          cloud_flavor: umcg.v1.vm.1-2
          host_networks:
            - name: "{{ stack_prefix }}_internal_management"
              security_group: "{{ stack_prefix }}_ds"
              assign_floating_ip: true
          local_volume_size_extra: 10000
#
# iRODS machines in Nibbler stack currently disabled as they are no longer used for the microscopy project
# and the RUG vulnerability scanner complains about the expired certificate on "umcg-icat01.hpc.rug.nl".
# Certificate must be renewed when we want to reuse these iRODS machines for other data management purposes.
#
#    irods:
#      hosts:
#        irods-catalogus:
#          fqdn: umcg-icat01.hpc.rug.nl
#          ir_yum_packages: '{{ ir_yum_packages_4_3_0 }}'   # test iRODS List of packages are defined in role defaults
#          pgsql_server: "remote"                            # PostgreSQL "local" or "remote" server
#          ir_local_res: 'rootResc'                          # iRODS local resource
#          ir_default_res: 'surfObjStore'                    # iRODS default resource for uploads
#          ir_zone: 'nlumcg'                                 # iRODS default main zone name
#          ir_negotiation_key: '{{ icatV_negotiation_key }}' # iRODS Vaulted negotiation key
#          ir_ctrl_plane_key: '{{ icatV_ctrl_plane_key }}'   # iRODS Vaulted control plane key
#          ir_zone_key: '{{ icatV_zone_key }}'               # iRODS Vaulted zone key
#          ir_db_user: '{{ icatV_db_user }}'                 # iRODS Vaulted db username
#          ir_db_pwd: '{{ icatV_db_pwd }}'                   # iRODS Vaulted pgsql password
#          ir_db_name: '{{ icatV_db_name }}'                 # iRODS Vaulted database name
#          ir_db_server: '{{ icatV_db_server }}'             # iRODS Vaulted database server
#          ir_db_port: '{{ icatV_db_port }}'                 # iRODS Vaulted database port
#          ir_salt: '{{ icatV_salt }}'                       # iRODS Vaulted salt key
#          ir_admin_name: 'rods'                             # iRODS (and zone) account
#          ir_admin_pwd: '{{ icatV_admin_pwd }}'             # iRODS Vaulted main admin password
#          tiering_install: False                            # True / False - if the playbook tiering.yml should be executed
#          # ir_local_stage_res: 'demoRescStage'              # Staging resource, before data moved to permanent resource
#          # ir_local_stage_res_fol: '/tmp/irods/{{ ir_local_stage_res }}'
#          # ir_local_perm_res: 'demoRescPerm'                # Permanent resource, where it will keep data indefinitely
#          # ir_local_perm_res_fol: '/tmp/irods/{{ ir_local_perm_res }}'
#          iptables_allow_ssh_outbound: []
#          iptables_allow_irods:
#            - "{{ all.ip_addresses['surfsara']['umcg-resc1'] }}"
#        irods-test:
#          fqdn: irods-test.hpc.rug.nl
#          yum_packages: '{{ yum_packages_4_2_11 }}'         # test iRODS List of packages are defined in role defaults
#          ir_client_server_policy: 'CS_NEG_REFUSE'          # test iRODS communicate (default) with SSL (CS_NEG_REQUIRE) or not (CS_NEG_REFUSE)
#          pgsql_server: "remote"                            # test iRODS PostgreSQL "local" or "remote" server
#          ir_local_res: 'demoResc'                          # test iRODS Staging resource, before data moved to permanent resource
#          ir_default_res: 'demoResc'                        # test iRODS default resource for uploads
#          ir_db_server: '127.0.0.1'                         # test iRODS pgsql server location
#          ir_db_pwd: '{{ icatV_test_db_pwd }}'              # test iRODS Vaulted pgsql password
#          ir_db_user: '{{ ir_service_account }}'            # test iRODS postgres user
#          ir_db_name: 'ICAT'                                # test iRODS database name
#          ir_admin_pwd: '{{ icatV_test_admin_pwd }}'        # test iRODS Vaulted irods admin password
#          ir_zone: 'tstzone'                                # test iRODS default main zone name
#          ir_salt: 'sA+dwq_dk29DJ1'                         # test iRODS salt
#          tiering_install: True                             # test iRODS True / False - if the playbook tiering.yml should be executed
#          ir_local_stage_res: 'demoRescStage'               # test iRODS staging resource, before data moved to permanent resource
#          ir_local_stage_res_fol: '/tmp/irods/{{ ir_local_stage_res }}' # test! iRODS staging resource folder path
#          ir_local_perm_res: 'demoRescPerm'                 # test! iRODS permanent resource, where it will keep data indefinitely
#          ir_local_perm_res_fol: '/tmp/irods/{{ ir_local_perm_res }}' # test! iRODS permanent resource folder path
#        irods-test-db:
#          fqdn: irods-test-db.hpc.rug.nl
#          cloud_flavor: m1.small                            # test-db CPU:1 MEM:2GB ROOTDISK:20GB
#          pgsql_server: "local"                             # test-db iRODS PostgreSQL "local" or "remote" server
#          ir_db_pwd: '{{ icatV_test_db_pwd }}'              # test-db iRODS Vaulted pgsql password
#          ir_db_user: '{{ ir_service_account }}'            # test-db iRODS postgres user
#          ir_db_name: 'ICAT'                                # test-db iRODS database name
#          ir_admin_pwd: '{{ icatV_test_admin_pwd }}'        # test-db iRODS Vaulted irods admin password
#      vars:
#        cloud_flavor: umcg.v1.vm.4-8
#        host_networks:
#          - name: "{{ stack_prefix }}_internal_management"
#            security_group: "{{ stack_prefix }}_irods"
#            assign_floating_ip: false  # No floating IP used from this network.
#          - name: "{{ stack_prefix }}_internal_management_13"
#            security_group: "{{ stack_prefix }}_irods"
#            assign_floating_ip: false  # Floating IP is used from this network, but assigned manually to be able to switch from production to test and back.
#        local_volume_size_extra: 20
#        davrods_icat_ip: "{{ ip_addresses[inventory_hostname]['nb_internal_management']['address'] }}"
    docs:
      hosts:
        docs-on-bateleur:
    sys_admin_interface:
      hosts:
        nb-sai:
          cloud_flavor: umcg.v1.vm.4-8
          host_networks:
            - name: "{{ stack_prefix }}_internal_management"
              security_group: "{{ stack_prefix }}_cluster"
            - name: "{{ stack_prefix }}_internal_storage"
              security_group: "{{ stack_prefix }}_storage"
            - name: vlan985
              security_group: "{{ stack_prefix }}_storage"
            - name: vlan1068
              security_group: "{{ stack_prefix }}_storage"
            - name: vlan1073
              security_group: "{{ stack_prefix }}_storage"
    deploy_admin_interface:
      hosts:
        nb-dai:
          cloud_flavor: umcg.v1.vm.4-8
          host_networks:
            - name: "{{ stack_prefix }}_internal_management"
              security_group: "{{ stack_prefix }}_cluster"
            - name: "{{ stack_prefix }}_internal_storage"
              security_group: "{{ stack_prefix }}_storage"
            - name: vlan985
              security_group: "{{ stack_prefix }}_storage"
            - name: vlan1068
              security_group: "{{ stack_prefix }}_storage"
            - name: vlan1073
              security_group: "{{ stack_prefix }}_storage"
          local_volume_size_extra: 3000
    user_interface:
      hosts:
        nibbler:
          cloud_flavor: umcg.v1.vm.16-64
          host_networks:
            - name: "{{ stack_prefix }}_internal_management"
              security_group: "{{ stack_prefix }}_cluster"
            - name: "{{ stack_prefix }}_internal_storage"
              security_group: "{{ stack_prefix }}_storage"
            - name: vlan985
              security_group: "{{ stack_prefix }}_storage"
            - name: vlan1068
              security_group: "{{ stack_prefix }}_storage"
            - name: vlan1073
              security_group: "{{ stack_prefix }}_storage"
          slurm_sockets: 16
          slurm_cores_per_socket: 1
          slurm_real_memory: 63790
          slurm_local_disk: 0
          slurm_features: 'prm02,prm03,tmp02'
          slurm_ethernet_interfaces:
            - ens3
            - ens4
            - ens5
            - ens6
    compute_node:
      children:
        regular:  # Must be item from {{ slurm_partitions }} variable defined in group_vars/{{ stack_name }}/vars.yml
          hosts:
            nb-node-a[01:05]:
          vars:
            cloud_flavor: umcg.v1.vm.63-500
            host_networks:
              - name: "{{ stack_prefix }}_internal_management"
                security_group: "{{ stack_prefix }}_cluster"
              - name: "{{ stack_prefix }}_internal_storage"
                security_group: "{{ stack_prefix }}_storage"
              - name: vlan985
                security_group: "{{ stack_prefix }}_storage"
              - name: vlan1068
                security_group: "{{ stack_prefix }}_storage"
              # DH5 not used on compute nodes (yet).
              #- name: vlan1073
              #  security_group: "{{ stack_prefix }}_storage"
            local_volume_size_extra: 1
            slurm_sockets: 63
            slurm_cores_per_socket: 1
            slurm_real_memory: 491355
            slurm_max_cpus_per_node: "{{ slurm_sockets * slurm_cores_per_socket - 2 }}"
            slurm_max_mem_per_node: "{{ slurm_real_memory - slurm_sockets * slurm_cores_per_socket * 512 }}"
            slurm_local_disk: 900
            slurm_features: 'tmp02'
            slurm_ethernet_interfaces:
              - ens3
              - ens4
              - ens5
              # DH5 not used on compute nodes (yet).
              #- ens6
        gpu_a40:  # Must be item from {{ slurm_partitions }} variable defined in group_vars/{{ stack_name }}/vars.yml
          hosts:
            nb-node-b[01:02]:
          vars:
            cloud_flavor: umcg.v1.vm.31-120.8a40
            gpu_count: 8
            gpu_type: 'a40'
            host_networks:
              - name: "{{ stack_prefix }}_internal_management"
                security_group: "{{ stack_prefix }}_cluster"
              - name: "{{ stack_prefix }}_internal_storage"
                security_group: "{{ stack_prefix }}_storage"
              - name: vlan985
                security_group: "{{ stack_prefix }}_storage"
              - name: vlan1068
                security_group: "{{ stack_prefix }}_storage"
              # DH5 not used on compute nodes (yet).
              #- name: vlan1073
              #  security_group: "{{ stack_prefix }}_storage"
            local_volume_size_extra: 1
            slurm_sockets: 31
            slurm_cores_per_socket: 1
            slurm_real_memory: 117398
            slurm_max_cpus_per_node: "{{ slurm_sockets * slurm_cores_per_socket - 2 }}"
            slurm_max_mem_per_node: "{{ slurm_real_memory - slurm_sockets * slurm_cores_per_socket * 512 }}"
            slurm_local_disk: 900
            slurm_features: 'tmp02,gpu,A40'
            slurm_weight: 10
            slurm_ethernet_interfaces:
              - ens3
              - ens4
              - ens5
              # DH5 not used on compute nodes (yet).
              #- ens6
administration:
  children:
    sys_admin_interface:
    deploy_admin_interface:
    user_interface:
cluster:
  children:
    compute_node:
    administration:
    #irods:
nibbler_cluster:
  children:
    openstack_api:
    #irods:
    jumphost:
    repo:
    cluster:
    data_transfer:
    docs:
...
